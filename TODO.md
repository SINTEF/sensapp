# SensApp Research Prototype Refactoring TODO

This document tracks the comprehensive refactoring plan for SensApp to create a focused research prototype for storage backend comparison.

## Phase 1: Remove Event Bus & Simplify Architecture (Week 1)

### üîÑ Architecture Simplification

- [ ] Add connection pooling for storage backends
- [x] Create proper storage factory with runtime selection via `settings.toml`
- [x] Simplify `main.rs` initialization
- [x] Keep ALL storage backends for research comparison

### ‚úÖ Phase 1 Completed

- [x] **Event Bus Removal**: Completely removed event bus from `main.rs` and all components
- [x] **Direct Storage Calls**: All components now use direct storage access instead of event bus
- [x] **Storage Factory**: Implemented runtime storage backend selection via connection strings
- [x] **Architecture Simplification**: `main.rs` initialization significantly simplified
- [x] **HttpServerState Refactored**: Now contains direct storage reference instead of event bus

## Phase 2: Add ClickHouse Storage Backend (Week 1)

### üóÑÔ∏è ClickHouse Implementation

- [ ] Create `src/storage/clickhouse/` module
- [ ] Implement ClickHouse storage trait
- [ ] Add ClickHouse migrations
- [ ] Include ClickHouse in storage factory
- [ ] Document ClickHouse-specific optimizations (LowCardinality, etc.)
- [ ] Add ClickHouse connection string parsing
- [ ] Test ClickHouse backend with sample data

### ‚úÖ Completed

- [ ] None yet

## Phase 3: Add Comprehensive Read API (Week 2)

### üìä Data Retrieval API

- [x] Implement metrics catalog endpoint (`GET /metrics`) with DCAT format
- [x] Implement series catalog endpoint (`GET /series`) with DCAT format
- [x] Implement series data endpoint (`GET /series/{uuid}`) with format selection
- [x] Support time range queries with `start/end` parameters
- [x] Support data format selection: CSV, JSON Lines, SenML, Apache Arrow
- [x] Add optional limit parameter for pagination
- [x] Add aggregation endpoints for research metrics (metrics catalog)
- [x] Implement data export in multiple formats (SenML, CSV, JSONL, Arrow)
- [ ] Add query performance metrics collection
- [ ] Add offset parameter for full pagination support

### ‚úÖ Phase 3 Completed

- [x] **DCAT Catalog Format**: Both metrics and series endpoints use W3C DCAT standard
- [x] **Multiple Export Formats**: SenML, CSV, JSON Lines, Apache Arrow all implemented
- [x] **Time Range Queries**: Full support for ISO 8601 datetime parsing with timezone handling
- [x] **UUID-based Series Access**: Clean UUID-based series identification and querying
- [x] **Prometheus-style IDs**: Series catalog includes Prometheus-compatible identifiers
- [x] **Rich Metadata**: Comprehensive sensor metadata with labels, units, and types

## Phase 4: Testing Infrastructure (Week 2)

### üß™ Unit Testing

- [ ] Unit tests for SQLite storage backend
- [ ] Unit tests for PostgreSQL storage backend
- [ ] Unit tests for TimescaleDB storage backend
- [ ] Unit tests for DuckDB storage backend
- [ ] Unit tests for BigQuery storage backend
- [ ] Unit tests for RRDCached storage backend
- [ ] Unit tests for ClickHouse storage backend (when implemented)
- [x] Unit tests for data type inference (`src/infer/`)
- [x] Unit tests for CSV parsing (`src/importers/`)
- [x] Unit tests for datetime parsing and timezone handling
- [x] Unit tests for export format handling and content types
- [x] Unit tests for Prometheus ID generation

### üîó Integration Testing

- [x] Integration tests for HTTP CRUD/DCAT endpoints (`tests/crud_dcat_api.rs`)
- [x] Integration tests for data ingestion (`tests/ingestion.rs`)
- [x] Integration tests for query and export functionality (`tests/query_export.rs`)
- [x] Integration tests for Apache Arrow export (`tests/arrow_integration.rs`)
- [x] Integration tests for datamodel edge cases (`tests/datamodel.rs`)
- [x] Integration tests for parser edge cases (`tests/parser_edge_cases.rs`)
- [ ] Integration tests for MQTT ingestion
- [ ] Integration tests for InfluxDB compatibility endpoints
- [ ] Integration tests for Prometheus compatibility endpoints
- [ ] Cross-storage backend data consistency tests

### ‚ö° Performance Benchmarks

- [ ] Write latency benchmarks per storage backend
- [ ] Read latency benchmarks per storage backend
- [ ] Storage space efficiency comparison
- [ ] Concurrent write performance tests
- [ ] Test harness for automated storage backend comparison

### ‚úÖ Phase 4 Completed

- [x] **Comprehensive Test Suite**: 6+ integration test files covering major functionality
- [x] **Export Format Testing**: All export formats (SenML, CSV, JSONL, Arrow) tested
- [x] **HTTP API Testing**: CRUD and DCAT API endpoints fully tested
- [x] **Data Model Testing**: Edge cases and type handling tested
- [x] **Parser Testing**: CSV and other format parsers tested

## Phase 5: Configuration & Observability (Week 3)

### ‚öôÔ∏è Configuration Management

- [x] Move ALL storage configs to `settings.toml`
- [x] Add runtime storage backend selection
- [ ] Add connection pooling configuration
- [ ] Add batch processing configuration
- [ ] Environment variable override support

### üìà Metrics & Monitoring

- [ ] Add Prometheus metrics endpoint (`/metrics`)
- [ ] Write latency metrics per storage backend
- [ ] Query latency metrics per storage backend
- [ ] Data ingestion rate metrics
- [ ] Storage-specific metrics (space usage, connections)
- [ ] Error rate metrics per backend
- [ ] Queue depth and processing metrics

### üìù Structured Logging

- [ ] Implement structured logging with `tracing`
- [ ] Log storage backend selection decisions
- [ ] Log performance metrics
- [ ] Log error details with context
- [ ] Add request tracing correlation IDs

### üè• Health Monitoring

- [ ] Health check endpoint (`/health`)
- [ ] Storage backend connectivity checks
- [ ] Database migration status checks
- [ ] Resource usage health indicators

### ‚úÖ Phase 5 Completed

- [x] **Storage Backend Selection**: Connection string based runtime selection working
- [x] **Settings Configuration**: All major settings moved to settings.toml
- [x] **Multi-Backend Support**: PostgreSQL, SQLite, DuckDB, BigQuery, TimescaleDB, RRDCached
- [x] **Network Configuration**: HTTP server endpoint and port configuration
- [x] **MQTT Configuration**: Optional MQTT client configuration support
- [x] **Sentry Integration**: Optional error tracking configuration

## Phase 6: Streamline Ingestion (Week 3)

### üîß Ingestion Simplification

- [x] Keep HTTP ingestion (all format support)
- [x] Keep MQTT for ingestion only
- [x] Remove OPC UA to separate crate/service
- [x] Remove AMQP planning/references
- [x] Optimize batch processing without event bus
- [ ] Add ingestion rate limiting
- [ ] Add backpressure handling

### ‚úÖ Phase 6 Completed

- [x] **Event Bus Removal**: Event bus completely removed from all components
- [x] **Direct Storage Access**: All ingestion now uses direct storage calls
- [x] **MQTT Simplification**: MQTT clients now have direct storage access
- [x] **Batch Processing**: Simplified batch processing without event bus overhead

## Phase 7: Research Tools & Documentation (Week 4)

### üî¨ Research Features

- [ ] Add storage backend comparison endpoints
- [ ] Create benchmark suite for storage comparison
- [ ] Add metrics dashboard configuration (Grafana)
- [ ] Performance comparison report generation
- [ ] Data consistency verification tools
- [ ] Storage backend switching without data loss

### üìö Documentation

- [ ] Document storage backend trade-offs
- [ ] Create research data collection guide
- [ ] Performance comparison methodology
- [ ] Deployment guide for each storage backend
- [ ] API documentation updates
- [ ] Configuration reference guide

### ‚úÖ Phase 7 Completed

- [x] **Export Format Support**: SenML, CSV, JSONL, Apache Arrow exporters implemented
- [x] **DCAT Catalog API**: W3C DCAT standard catalog endpoints for research compatibility
- [x] **Storage Backend Comparison**: All storage backends maintained for research
- [x] **Data Format Flexibility**: Multiple ingestion and export formats supported

## Key Principles for Research Prototype

- ‚úÖ Maintain all storage backends for comparison
- ‚úÖ Focus on measurement and observability
- ‚úÖ Make storage selection runtime configurable
- ‚úÖ Prioritize research flexibility over production optimization
- ‚úÖ Keep ingestion simple (HTTP + MQTT only)
- ‚úÖ Remove unnecessary complexity (event bus, OPC UA)

## Session Notes

### Session 1: Analysis and Planning (Current)

**Completed:**

- Created comprehensive project plan and TODO document
- Analyzed event bus usage patterns across codebase
- Identified all locations where event bus is used for storage calls
- Designed direct storage call pattern to replace event bus
- Started refactoring: Updated `HttpServerState` to remove event bus dependency

**Key Findings:**

- Event bus creates single-threaded bottleneck in `main.rs:178-210`
- All storage calls are serialized through single consumer task
- Event bus is used in 11 files: CSV importers, MQTT/OPC UA clients, HTTP endpoints, batch builder
- Current pattern: `event_bus.publish(batch)` ‚Üí single consumer ‚Üí `storage.publish(batch)`
- New pattern: Direct `storage.publish(batch)` calls for parallel processing

**What Has Been Accomplished:**

Major refactoring completed across multiple phases:

### Session 2-17: Major Implementation Work (üåä waves 2-17)

**Event Bus Removal & Architecture Simplification (‚úÖ COMPLETED)**

- Completely removed event bus from main.rs and all components
- Updated all HTTP endpoints to use direct storage access
- Refactored MQTT clients to use direct storage calls
- Simplified main.rs initialization significantly

**Comprehensive Read API (‚úÖ COMPLETED)**

- Implemented DCAT-compliant catalog endpoints: `/metrics` and `/series`
- Added full series data endpoint: `/series/{uuid}` with format selection
- Support for multiple export formats: SenML, CSV, JSONL, Apache Arrow
- Time range queries with ISO 8601 datetime parsing and timezone handling

**Testing Infrastructure (‚úÖ MAJOR PROGRESS)**

- 6+ comprehensive integration test files implemented
- All export formats tested (SenML, CSV, JSONL, Arrow)
- HTTP API endpoints fully tested
- Data model and parser edge cases tested

**Configuration & Settings (‚úÖ COMPLETED)**

- All storage backends configured via settings.toml
- Runtime storage backend selection working
- MQTT, Sentry, and network configuration implemented

**Research-Ready Features (‚úÖ COMPLETED)**

- All storage backends maintained for comparison
- DCAT catalog format for research compatibility
- Multiple data format support for flexibility

### Next Session Plan

**High Priority Remaining Items:**

- [ ] ClickHouse storage backend implementation (Phase 2)
- [ ] Performance benchmarks for storage backend comparison
- [ ] Health check and metrics endpoints (/health, /metrics)
- [ ] Connection pooling configuration

**Medium Priority:**

- [ ] Storage backend unit tests
- [ ] Query performance metrics collection
- [ ] Rate limiting and backpressure handling

- Antoine's manual TODO:

- query_sensor_data only supports floats, not good.
- fix the automatic inference of CSV
- continue implementing the prometheus remote read API.
- consider removing the whole inference thing.
- adding support for prometheus like query syntax and sensor names in data imports / exports.
- does strict mode have an unit support ?¬†it should.
